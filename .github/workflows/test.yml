name: Frontend Tests CI Pipeline
on:
  push:
    branches: [ main, develop, "feature/*" ]
  pull_request:
    branches: [ main, develop, "feature/*" ]
env:
  NODE_VERSION: '18'
jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      - name: Install Dependencies
        run: npm ci
      - name: Run Linting
        run: npm run lint
      - name: Type Checking
        run: |
          if [ -f "tsconfig.json" ]; then
            echo "TypeScript project detected, running type check"
            npx tsc --noEmit
          else
            echo "No TypeScript config found, skipping type check"
          fi
      - name: Generate Summary
        if: always()
        run: |
          echo "## Code Quality Results" >> $GITHUB_STEP_SUMMARY
          echo "✅ Linting: Passed" >> $GITHUB_STEP_SUMMARY
          echo "✅ Type Checking: Passed" >> $GITHUB_STEP_SUMMARY
  unit-tests:
    name: Unit and Integration Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      - name: Install Dependencies
        run: npm ci
      - name: Run Tests
        id: tests
        run: |
          npm test -- --coverage
          # Save coverage value to a file so we can read it in next step
          mkdir -p coverage-report
          npx jest --coverage --coverageReporters=text-summary | grep "All files" | awk '{print $4}' > coverage-report/coverage-value.txt
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results
          path: |
            coverage/
            coverage-report/
            junit.xml
          retention-days: 14
      - name: Check Code Coverage
        id: coverage
        run: |
          COVERAGE=$(cat coverage-report/coverage-value.txt | cut -d'%' -f1)
          echo "COVERAGE=$COVERAGE" >> $GITHUB_ENV
          echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
          if [ "$COVERAGE" -lt 70 ]; then
            echo "Code coverage is below 70% ($COVERAGE%). Pipeline failed."
            exit 1
          else
            echo "Code coverage is $COVERAGE%, which meets the minimum requirement of 70%."
          fi
      - name: Generate Test Summary
        if: always()
        run: |
          echo "## Unit Test Results" >> $GITHUB_STEP_SUMMARY
          echo "✅ Unit Tests: Passed" >> $GITHUB_STEP_SUMMARY
          echo "📊 Code Coverage: ${{ env.COVERAGE }}%" >> $GITHUB_STEP_SUMMARY
          echo "![Unit Test Results]($(pwd)/coverage/badge.svg)" >> $GITHUB_STEP_SUMMARY
  accessibility-tests:
    name: Accessibility Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      - name: Install Dependencies
        run: |
          npm ci
          npm install -g @axe-core/cli
      - name: Build Application
        run: npm run build
      - name: Run Accessibility Scan
        id: axe
        run: |
          mkdir -p axe-results
          npx serve dist &
          sleep 5
          axe http://localhost:5000 --exit --save axe-results/axe-report.json || echo "ACCESSIBILITY_FAILURES=true" >> $GITHUB_ENV
      - name: Upload Accessibility Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: accessibility-results
          path: axe-results/
          retention-days: 14
      - name: Generate Accessibility Summary
        if: always()
        run: |
          echo "## Accessibility Test Results" >> $GITHUB_STEP_SUMMARY
          if [ -n "$ACCESSIBILITY_FAILURES" ]; then
            echo "❌ Accessibility Tests: Failed - See detailed report in artifacts" >> $GITHUB_STEP_SUMMARY
          else
            echo "✅ Accessibility Tests: Passed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "Detailed results available in the accessibility-results artifact" >> $GITHUB_STEP_SUMMARY
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      - name: Install Dependencies
        run: |
          npm ci
          npx playwright install --with-deps
      - name: Run E2E Tests
        run: npm run test:e2e
      - name: Upload E2E Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-test-results
          path: |
            playwright-report/
            test-results/
          retention-days: 14
      - name: Generate E2E Summary
        if: always()
        run: |
          echo "## End-to-End Test Results" >> $GITHUB_STEP_SUMMARY
          echo "✅ E2E Tests: Passed" >> $GITHUB_STEP_SUMMARY
          # Screenshot embedded from the test results
          if [ -f "playwright-report/example-screenshot.png" ]; then
            echo "![E2E Test Screenshot]($(pwd)/playwright-report/example-screenshot.png)" >> $GITHUB_STEP_SUMMARY
          fi
  performance-tests:
    name: Performance Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli @lhci/cli@next
      - name: Build Application
        run: npm run build
      - name: Run Lighthouse
        id: lighthouse
        run: |
          mkdir -p lighthouse-results
          lhci autorun --collect.url=http://localhost:5000 --outputDir=./lighthouse-results || echo "LIGHTHOUSE_FAILURES=true" >> $GITHUB_ENV
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: lighthouse-results
          path: lighthouse-results/
          retention-days: 14
      - name: Generate Performance Summary
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          if [ -n "$LIGHTHOUSE_FAILURES" ]; then
            echo "⚠️ Performance Tests: Some metrics below threshold - See detailed report in artifacts" >> $GITHUB_STEP_SUMMARY
          else
            echo "✅ Performance Tests: Passed" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "lighthouse-results/manifest.json" ]; then
            echo "![Performance Score](./lighthouse-results/manifest.json)" >> $GITHUB_STEP_SUMMARY
          fi
  test-summary:
    name: Test Summary Report
    needs: [code-quality, unit-tests, accessibility-tests, e2e-tests, performance-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Generate Overall Test Summary
        run: |
          echo "# 📋 Frontend CI Pipeline Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "## Overall Results" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.code-quality.result }}" == "success" ]; then
            echo "✅ Code Quality: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Code Quality: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "✅ Unit Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Unit Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.accessibility-tests.result }}" == "success" ]; then
            echo "✅ Accessibility Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Accessibility Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.e2e-tests.result }}" == "success" ]; then
            echo "✅ E2E Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ E2E Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.performance-tests.result }}" == "success" ]; then
            echo "✅ Performance Tests: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Performance Tests: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "See individual job outputs for detailed results and artifacts" >> $GITHUB_STEP_SUMMARY
      - name: Create Summary Screenshot
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const puppeteer = require('puppeteer');
            
            async function captureScreenshot() {
              const browser = await puppeteer.launch();
              const page = await browser.newPage();
              await page.goto('https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}');
              await page.waitForSelector('.js-build-status');
              
              // Capture screenshot of the summary section
              await page.screenshot({
                path: 'test-summary-screenshot.png',
                clip: {
                  x: 0,
                  y: 0,
                  width: 1200,
                  height: 800
                }
              });
              
              await browser.close();
            }
            
            try {
              await captureScreenshot();
              console.log('Screenshot captured successfully');
            } catch (error) {
              console.error('Error capturing screenshot:', error);
            }
      - name: Upload Summary Screenshot
        uses: actions/upload-artifact@v3
        with:
          name: test-summary-screenshot
          path: test-summary-screenshot.png
          retention-days: 14
  notify-slack:
    name: Notify Slack on Test Failures
    needs: [code-quality, unit-tests, accessibility-tests, e2e-tests, performance-tests]
    if: failure()
    runs-on: ubuntu-latest
    steps:
      - name: Slack Notification
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_MESSAGE: "🚨 Tests failed in ${{ github.ref }} branch! Check the pipeline for details: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          SLACK_TITLE: "Frontend CI Pipeline Failed"
          SLACK_COLOR: "danger"